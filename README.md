# ğŸ”¢ MNIST Digit Recognition with Deep Learning

This notebook implements a comprehensive deep learning solution for handwritten digit recognition using the MNIST dataset, applying neural networks with TensorFlow/Keras to achieve high accuracy in digit classification.

---

## ğŸ“š Table of Contents

- [ğŸ’» Installation](#-installation)
- [ğŸ¯ Project Goals](#-project-goals)
- [ğŸ§  Methods](#-methods)
- [ğŸ“ˆ Results](#-results)
- [ğŸ” Key Insights](#-key-insights)
- [ğŸ§¾ Project Details](#-project-details)
- [ğŸªª License](#-license)

---

## ğŸ’» Installation

- **Requirements:** Python 3.8+, Jupyter Notebook, TensorFlow 2.x

```bash
# Clone the repository
git clone https://github.com/alhussienhazem/mnist-digit-recognition.git

# Navigate to the project folder
cd mnist-digit-recognition

# (optional) create a virtual environment
python -m venv venv
source venv/bin/activate  # on Linux/Mac
venv\Scripts\activate     # on Windows

# Install dependencies
pip install -r requirements.txt

# Run the notebook
jupyter notebook
```

---

## ğŸ¯ Project Goals

- **Objective:** Build a deep learning model to recognize handwritten digits (0-9) from the MNIST dataset
- **Challenge:** Achieve high accuracy in digit classification while handling class imbalance
- **Solution:** Implement a neural network with proper data preprocessing and class balancing
- **Features Used:** 28x28 pixel grayscale images of handwritten digits
- **Target:** Multi-class classification (10 classes: digits 0-9)

---

## ğŸ§  Methods

- **Models Used:**
  - ğŸ§  Sequential Neural Network
  - ğŸ”¢ Multi-layer Perceptron (MLP)
  - âš–ï¸ Class Balancing with RandomUnderSampler

- **Techniques Applied:**
  - ğŸ” Comprehensive data visualization and exploration
  - ğŸ“Š Class distribution analysis and imbalance handling
  - ğŸ”§ Data normalization (0-1 scaling)
  - ğŸ§  Neural network architecture with multiple dense layers
  - âš¡ Adam optimizer with sparse categorical crossentropy loss
  - ğŸ“ˆ Early stopping and validation monitoring
  - ğŸ¯ Confusion matrix and classification report analysis
  - ğŸ“Š Training history visualization

---

## ğŸ“ˆ Results

### **Model Performance**
| Metric | Score | Performance |
|--------|-------|-------------|
| ğŸ¯ Accuracy | 98.22% | **Excellent Performance** |
| ğŸ“Š Precision | 98.0% | High Precision |
| ğŸ”„ Recall | 98.0% | High Recall |
| âš–ï¸ F1-Score | 98.0% | Balanced Performance |

---

### **Model Architecture**
| Layer | Type | Units | Activation |
|-------|------|-------|------------|
| ğŸ”¢ Input | Flatten | 784 | - |
| ğŸ§  Hidden 1 | Dense | 1025 | ReLU |
| ğŸ§  Hidden 2 | Dense | 512 | ReLU |
| ğŸ§  Hidden 3 | Dense | 256 | ReLU |
| ğŸ¯ Output | Dense | 10 | Softmax |

---

### **ğŸ” Key Insights**
- **High Accuracy**: Achieved 98.22% accuracy on test set, demonstrating excellent digit recognition capability
- **Class Balance**: Successfully handled class imbalance using RandomUnderSampler technique
- **Data Preprocessing**: Proper normalization (0-1 scaling) significantly improved model performance
- **Neural Network Design**: Multi-layer architecture with ReLU activations proved effective for digit classification
- **Training Efficiency**: Model converged quickly with 10 epochs, showing good learning dynamics
- **Visualization**: Comprehensive visualization of training progress and prediction results
- **Robust Performance**: Consistent high performance across all digit classes (0-9)

---

## ğŸ§¾ Project Details

**ğŸ”§ Enhanced Data Preprocessing:**
- MNIST dataset loading and exploration
- Data shape analysis and validation
- Class distribution visualization
- Random under-sampling for class balance
- Data normalization (0-1 scaling)

**ğŸ“Š Exploratory Data Analysis:**
- Sample digit visualization
- Class distribution analysis
- Data shape and structure exploration
- Comprehensive digit sample display
- Statistical summary analysis

**ğŸ§  Neural Network Implementation:**
- Sequential model architecture
- Multiple dense layers with ReLU activation
- Softmax output layer for multi-class classification
- Adam optimizer with sparse categorical crossentropy
- Batch training with validation monitoring

**ğŸ“ˆ Model Evaluation:**
- Accuracy, precision, recall, and F1-score metrics
- Confusion matrix visualization
- Classification report analysis
- Training history plots (accuracy and loss)
- Prediction visualization with confidence scores

**ğŸ“‹ Notebook Structure:**
- Data Loading & Initial Exploration
- Data Visualization & Analysis
- Class Imbalance Handling
- Data Preprocessing & Normalization
- Neural Network Architecture
- Model Training & Validation
- Performance Evaluation & Visualization

---

## ğŸªª License

This project is for educational and non-commercial use only.  
Dataset source: [TensorFlow MNIST Dataset](https://www.tensorflow.org/datasets/catalog/mnist) 